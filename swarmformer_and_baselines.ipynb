{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7966535",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2930a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp \"/content/drive/MyDrive/converted_datasets.zip\" /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e3000",
   "metadata": {},
   "outputs": [],
   "source": [
    "drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a75ea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf \"/content/converted_datasets.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1b5972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c47ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def compute_cvar(ranges, range_max, alpha=0.05):\n",
    "    valid = ranges[np.isfinite(ranges)]\n",
    "    if valid.size == 0 or range_max == 0:\n",
    "        return 1.0\n",
    "    n = max(1, int(alpha * valid.size))\n",
    "    cvar = np.sort(valid)[:n].mean()\n",
    "    return float(np.clip(1.0 - (cvar / range_max), 0.0, 1.0))\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=3500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                        -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class ModalityEncoder(nn.Module):\n",
    "    def __init__(self, inp_dim, d_model, nhead, layers, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(inp_dim, d_model)\n",
    "        self.pos = PositionalEncoding(d_model)\n",
    "        layer = nn.TransformerEncoderLayer(d_model, nhead, d_model * 4, dropout, batch_first=True)\n",
    "        self.enc = nn.TransformerEncoder(layer, layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.proj(x) * (self.proj.out_features ** 0.5)\n",
    "        x = self.pos(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.enc(x, src_key_padding_mask=mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, nhead, dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, q, k, v, kmask=None):\n",
    "        out, _ = self.attn(q, k, v, key_padding_mask=kmask)\n",
    "        q = self.norm1(q + self.drop(out))\n",
    "        ff = self.ff(q)\n",
    "        return self.norm2(q + self.drop(ff))\n",
    "\n",
    "\n",
    "class GMMHead(nn.Module):\n",
    "    def __init__(self, d_model, act_dim, comps=5):\n",
    "        super().__init__()\n",
    "        self.K = comps\n",
    "        self.ad = act_dim\n",
    "        self.logits = nn.Linear(d_model, comps)\n",
    "        self.means = nn.Linear(d_model, comps * act_dim)\n",
    "        self.log_stds = nn.Linear(d_model, comps * act_dim)\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        logits = self.logits(x)\n",
    "        means = self.means(x).view(B, T, self.K, self.ad)\n",
    "        logstd = self.log_stds(x).view(B, T, self.K, self.ad)\n",
    "        stds = torch.exp(logstd.clamp(-5, 2))\n",
    "        return logits, means, stds\n",
    "\n",
    "\n",
    "class FormationGoalEncoder(nn.Module):\n",
    "    def __init__(self, goal_dim=13, d_model=256, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(goal_dim, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.enc(x)\n",
    "\n",
    "\n",
    "class SwarmTransformerWithGoal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.odom_enc = ModalityEncoder(13, 256, 8, 4)\n",
    "        self.scan_enc = ModalityEncoder(361, 256, 8, 4)\n",
    "        self.gossip_enc = ModalityEncoder(70, 256, 8, 4)\n",
    "        self.goal_enc = FormationGoalEncoder(goal_dim=13, d_model=256)\n",
    "        self.cross1 = CrossModalAttention(256, 8)  # State + Goal\n",
    "        self.cross2 = CrossModalAttention(256, 8)  # Goal-aware + Perception\n",
    "        self.cross3 = CrossModalAttention(256, 8)  # Final + Communication\n",
    "        self.gmm_head = GMMHead(256, 2, comps=5)\n",
    "    def forward(self, odom, scan, gossip, formation_goal, smask, gmask):\n",
    "        o = self.odom_enc(odom)              # [B, T, 256]\n",
    "        s = self.scan_enc(scan, smask)       # [B, T, 256]\n",
    "        g = self.gossip_enc(gossip, gmask)   # [B, T, 256]\n",
    "        fg = self.goal_enc(formation_goal)   # [B, T, 256]\n",
    "\n",
    "        # Stage 1: Fuse state with goal\n",
    "        goal_aware_state = self.cross1(o, fg, fg, None)\n",
    "        # Stage 2: Fuse with perception\n",
    "        perception_fusion = self.cross2(goal_aware_state, s, s, smask)\n",
    "        # Stage 3: Fuse with communication/gossip\n",
    "        full = self.cross3(perception_fusion, g, g, gmask)\n",
    "\n",
    "        return self.gmm_head(full)\n",
    "    def loss(self, logits, means, stds, tgt):\n",
    "        B, T, K, D = means.shape\n",
    "        lf = logits.view(-1, K)\n",
    "        mf = means.view(-1, K, D)\n",
    "        sf = stds.view(-1, K, D)\n",
    "        tf = tgt.view(-1, D)\n",
    "        mix = torch.distributions.Categorical(logits=lf)\n",
    "        comp = torch.distributions.Independent(torch.distributions.Normal(mf, sf), 1)\n",
    "        gmm = torch.distributions.MixtureSameFamily(mix, comp)\n",
    "        return -gmm.log_prob(tf).mean()\n",
    "\n",
    "\n",
    "class SwarmRobotDataset(Dataset):\n",
    "    def __init__(self, files, max_nb=10, scan_dim=360, seq_len=None, normalize=True):\n",
    "        self.files = files\n",
    "        self.max_nb = max_nb\n",
    "        self.scan_dim = scan_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.normalize = normalize\n",
    "        self.samples = []\n",
    "        for f in files:\n",
    "            data = np.load(f, allow_pickle=True)\n",
    "            for robot in data.files:\n",
    "                self.samples.append((f, robot))\n",
    "        assert self.samples, \"No samples loaded\"\n",
    "        if self.normalize:\n",
    "            self._compute_stats()\n",
    "            logger.info(f\"Computed stats on {len(self.samples)} samples\")\n",
    "            logger.info(f\"odom_mean: {self.om.tolist()}, odom_std: {self.os.tolist()}\")\n",
    "            logger.info(f\"scan_mean (first5): {self.sm.tolist()[:5]}, scan_std (first5): {self.ss.tolist()[:5]}\")\n",
    "            logger.info(f\"gossip_mean: {self.gm.tolist()}, gossip_std: {self.gs.tolist()}\")\n",
    "            logger.info(f\"cmd_vel_mean: {self.cm.tolist()}, cmd_vel_std: {self.cs.tolist()}\")\n",
    "\n",
    "    def _compute_stats(self):\n",
    "        od, sc, go, cmd = [], [], [], []\n",
    "        for f, robot in random.sample(self.samples, min(len(self.samples), 1000)):\n",
    "            odom, scan, gossip, ms, mg, cmdv = self._load_one(f, robot)\n",
    "            od.append(odom)\n",
    "            sc.append(scan)\n",
    "            go.append(gossip)\n",
    "            cmd.append(cmdv)\n",
    "        od = torch.cat(od, 0)\n",
    "        sc = torch.cat(sc, 0)\n",
    "        go = torch.cat(go, 0)\n",
    "        cmd = torch.cat(cmd, 0)\n",
    "        self.om, self.os = od.mean(0), od.std(0) + 1e-8\n",
    "        self.sm, self.ss = sc.mean(0), sc.std(0) + 1e-8\n",
    "        self.gm, self.gs = go.mean(0), go.std(0) + 1e-8\n",
    "        self.cm, self.cs = cmd.mean(0), cmd.std(0) + 1e-8\n",
    "\n",
    "    def _load_one(self, f, robot):\n",
    "        data = np.load(f, allow_pickle=True)\n",
    "        rd = data[robot].item()\n",
    "        odom = torch.tensor(rd['odom'], dtype=torch.float32)\n",
    "        cmdv = torch.tensor(rd['cmd_vel'], dtype=torch.float32)\n",
    "        scan_list, mask_s = [], []\n",
    "        for r in rd['scan']:\n",
    "            if r is None:\n",
    "                scan_list.append(np.zeros(self.scan_dim + 1))\n",
    "                mask_s.append(True)\n",
    "            else:\n",
    "                array = r.astype(np.float32)\n",
    "                valid = array[np.isfinite(array)]\n",
    "                range_max = valid.max() if valid.size > 0 else 1.0\n",
    "                cvar = compute_cvar(array, float(range_max))\n",
    "                array = np.where(np.isinf(array), range_max, array) / range_max\n",
    "                if array.size != self.scan_dim:\n",
    "                    array = np.interp(np.linspace(0, array.size - 1, self.scan_dim),\n",
    "                                      np.arange(array.size), array)\n",
    "                scan_list.append(np.concatenate(([cvar], array)))\n",
    "                mask_s.append(False)\n",
    "        scan = torch.tensor(np.vstack(scan_list), dtype=torch.float32)\n",
    "        mask_s = torch.tensor(mask_s)\n",
    "        go_list, mask_g = [], []\n",
    "        for g in rd['gossip']:\n",
    "            if g.size == 0:\n",
    "                go_list.append(np.zeros(self.max_nb * 7))\n",
    "                mask_g.append(True)\n",
    "            else:\n",
    "                arr = np.zeros(self.max_nb * 7)\n",
    "                for i, row in enumerate(g[:self.max_nb]):\n",
    "                    arr[i * 7:(i + 1) * 7] = row\n",
    "                go_list.append(arr)\n",
    "                mask_g.append(False)\n",
    "        gossip = torch.tensor(np.vstack(go_list), dtype=torch.float32)\n",
    "        mask_g = torch.tensor(mask_g)\n",
    "        return odom, scan, gossip, mask_s, mask_g, cmdv\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        f, robot = self.samples[idx]\n",
    "        odom, scan, gossip, ms, mg, cmdv = self._load_one(f, robot)\n",
    "        L = odom.size(0)\n",
    "        if self.seq_len and L > self.seq_len:\n",
    "            i = random.randint(0, L - self.seq_len)\n",
    "            odom, scan, gossip, ms, mg, cmdv = [\n",
    "                x[i:i + self.seq_len] for x in (odom, scan, gossip, ms, mg, cmdv)\n",
    "            ]\n",
    "        if self.normalize:\n",
    "            odom = (odom - self.om) / self.os\n",
    "            scan = (scan - self.sm) / self.ss\n",
    "            gossip = (gossip - self.gm) / self.gs\n",
    "            cmdv = (cmdv - self.cm) / self.cs\n",
    "\n",
    "        formation_goal_vec = odom[-1].unsqueeze(0).repeat(odom.size(0), 1)  # [L, 13]\n",
    "        return {\n",
    "            'odom': odom, 'scan': scan, 'gossip': gossip,\n",
    "            'scan_mask': ms, 'gossip_mask': mg, 'cmd_vel': cmdv,\n",
    "            'formation_goal': formation_goal_vec\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    B = len(batch)\n",
    "    L = max(x['odom'].size(0) for x in batch)\n",
    "    odom = torch.zeros(B, L, 13)\n",
    "    scan = torch.zeros(B, L, 361)\n",
    "    gossip = torch.zeros(B, L, 70)\n",
    "    sm = torch.ones(B, L, dtype=torch.bool)\n",
    "    gm = torch.ones(B, L, dtype=torch.bool)\n",
    "    cmdv = torch.zeros(B, L, 2)\n",
    "    formation_goal = torch.zeros(B, L, 13)\n",
    "    for i, x in enumerate(batch):\n",
    "        l = x['odom'].size(0)\n",
    "        odom[i, :l] = x['odom']\n",
    "        scan[i, :l] = x['scan']\n",
    "        gossip[i, :l] = x['gossip']\n",
    "        sm[i, :l] = x['scan_mask']\n",
    "        gm[i, :l] = x['gossip_mask']\n",
    "        cmdv[i, :l] = x['cmd_vel']\n",
    "        formation_goal[i, :l] = x['formation_goal']\n",
    "    return {\n",
    "        'odom': odom, 'scan': scan, 'gossip': gossip,\n",
    "        'scan_mask': sm, 'gossip_mask': gm, 'cmd_vel': cmdv,\n",
    "        'formation_goal': formation_goal\n",
    "    }\n",
    "\n",
    "\n",
    "class SwarmTrainer:\n",
    "    def __init__(self, model, tl, vl, te, device):\n",
    "        self.m, self.tl, self.vl, self.te = model.to(device), tl, vl, te\n",
    "        self.dev = device\n",
    "        self.opt = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "        self.sch = torch.optim.lr_scheduler.CosineAnnealingLR(self.opt, T_max=100)\n",
    "        self.best = float('inf')\n",
    "        self.train_losses, self.val_losses = [], []\n",
    "        self.train_maes, self.val_maes = [], []\n",
    "        self.train_rmses, self.val_rmses = [], []\n",
    "\n",
    "    def _run(self, loader, train):\n",
    "        self.m.train() if train else self.m.eval()\n",
    "        total_loss, total_mae, total_rmse, n_samples = 0.0, 0.0, 0.0, 0\n",
    "        for b in tqdm(loader, desc=\"Train\" if train else \"Val\"):\n",
    "            od, sc, go, fg, sm, gm, cmd = [b[k].to(self.dev) for k in\n",
    "                                           ('odom', 'scan', 'gossip', 'formation_goal', 'scan_mask', 'gossip_mask', 'cmd_vel')]\n",
    "            with torch.set_grad_enabled(train):\n",
    "                logits, means, stds = self.m(od, sc, go, fg, sm, gm)\n",
    "                loss = self.m.loss(logits, means, stds, cmd)\n",
    "                if train:\n",
    "                    self.opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.m.parameters(), 1.0)\n",
    "                    self.opt.step()\n",
    "            total_loss += loss.item()\n",
    "            n_samples += 1\n",
    "            maxidx = logits.argmax(dim=-1)\n",
    "            pred = torch.gather(means, 2, maxidx.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, 1, means.size(-1))).squeeze(2)\n",
    "            total_mae += (pred - cmd).abs().mean().item()\n",
    "            total_rmse += ((pred - cmd) ** 2).mean().sqrt().item()\n",
    "        avg_loss = total_loss / n_samples\n",
    "        avg_mae = total_mae / n_samples\n",
    "        avg_rmse = total_rmse / n_samples\n",
    "        return avg_loss, avg_mae, avg_rmse\n",
    "\n",
    "    def train(self, epochs=100):\n",
    "        for e in range(1, epochs + 1):\n",
    "            tr_loss, tr_mae, tr_rmse = self._run(self.tl, True)\n",
    "            val_loss, val_mae, val_rmse = self._run(self.vl, False)\n",
    "            self.train_losses.append(tr_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.train_maes.append(tr_mae)\n",
    "            self.val_maes.append(val_mae)\n",
    "            self.train_rmses.append(tr_rmse)\n",
    "            self.val_rmses.append(val_rmse)\n",
    "\n",
    "            if val_loss < self.best:\n",
    "                self.best = val_loss\n",
    "                torch.save(self.m.state_dict(), f\"best_form_model_{e}.pth\")\n",
    "\n",
    "            self.sch.step()\n",
    "            print()\n",
    "            print(f\"Epoch: {e}/{epochs} | Train Loss: {tr_loss:.4f} | Val Loss: {val_loss:.4f} \"\n",
    "                  f\"Train MAE: {tr_mae:.4f} | Val MAE:{val_mae:.4f} \"\n",
    "                  f\"Train RMSE {tr_rmse:.4f} | Val RMSE {val_rmse:.4f}\", flush=True)\n",
    "            logger.info(f\"Epoch {e}/{epochs} Train Loss {tr_loss:.4f} Val Loss {val_loss:.4f} \"\n",
    "                        f\"Train MAE {tr_mae:.4f} Val MAE {val_mae:.4f} \"\n",
    "                        f\"Train RMSE {tr_rmse:.4f} Val RMSE {val_rmse:.4f}\")\n",
    "\n",
    "        plt.figure(); plt.plot(self.train_losses, label='Train'); plt.plot(self.val_losses, label='Val'); plt.legend()\n",
    "        plt.savefig(\"training_curve.png\")\n",
    "        te_loss, te_mae, te_rmse = self._run(self.te, False)\n",
    "        logger.info(f\"Test Loss: {te_loss:.4f} | Test MAE: {te_mae:.4f} | Test RMSE: {te_rmse:.4f}\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"------------------ Starting Code -----------------------\")\n",
    "    print(\"Device : \", DEVICE)\n",
    "    DATA_DIR = \"/content/converted_datasets\"\n",
    "    files = list(Path(DATA_DIR).glob(\"*.npz\"))\n",
    "    tr, tmp = train_test_split(files, test_size=0.3, random_state=42)\n",
    "    vl, te = train_test_split(tmp, test_size=0.5, random_state=42)\n",
    "    print(\"Splits made:-\")\n",
    "    print(f\"Train: {len(tr)} | Val: {len(vl)} | Test: {len(te)}\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Creating Dataset Classes.\")\n",
    "    train_ds = SwarmRobotDataset(tr, seq_len=3500, normalize=True)\n",
    "    print(\"Train Dataset Class created.\")\n",
    "    val_ds = SwarmRobotDataset(vl, seq_len=3500, normalize=False)\n",
    "    print(\"Val Dataset Class created.\")\n",
    "    test_ds = SwarmRobotDataset(te, seq_len=3500, normalize=False)\n",
    "    print(\"Test Dataset Class created.\")\n",
    "    print((f\"Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\"))\n",
    "    for ds in (val_ds, test_ds):\n",
    "        ds.om, ds.os = train_ds.om, train_ds.os\n",
    "        ds.sm, ds.ss = train_ds.sm, train_ds.ss\n",
    "        ds.gm, ds.gs = train_ds.gm, train_ds.gs\n",
    "        ds.cm, ds.cs = train_ds.cm, train_ds.cs\n",
    "        ds.normalize = True\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "    model = SwarmTransformerWithGoal()\n",
    "    logger.info(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Starting Training.\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    trainer = SwarmTrainer(model, train_loader, val_loader, test_loader, DEVICE)\n",
    "    trainer.train(epochs=100)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed515678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def compute_cvar(ranges, range_max, alpha=0.05):\n",
    "    valid = ranges[np.isfinite(ranges)]\n",
    "    if valid.size == 0 or range_max == 0:\n",
    "        return 1.0\n",
    "    n = max(1, int(alpha * valid.size))\n",
    "    cvar = np.sort(valid)[:n].mean()\n",
    "    return float(np.clip(1.0 - (cvar / range_max), 0.0, 1.0))\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=3500):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() *\n",
    "                        -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class ModalityEncoder(nn.Module):\n",
    "    def __init__(self, inp_dim, d_model, nhead, layers, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(inp_dim, d_model)\n",
    "        self.pos = PositionalEncoding(d_model)\n",
    "        layer = nn.TransformerEncoderLayer(d_model, nhead, d_model * 4, dropout, batch_first=True)\n",
    "        self.enc = nn.TransformerEncoder(layer, layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.proj(x) * (self.proj.out_features ** 0.5)\n",
    "        x = self.pos(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.enc(x, src_key_padding_mask=mask)\n",
    "        return self.norm(x)\n",
    "\n",
    "\n",
    "class CrossModalAttention(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, nhead, dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, q, k, v, kmask=None):\n",
    "        out, _ = self.attn(q, k, v, key_padding_mask=kmask)\n",
    "        q = self.norm1(q + self.drop(out))\n",
    "        ff = self.ff(q)\n",
    "        return self.norm2(q + self.drop(ff))\n",
    "\n",
    "\n",
    "class GMMHead(nn.Module):\n",
    "    def __init__(self, d_model, act_dim, comps=5):\n",
    "        super().__init__()\n",
    "        self.K = comps\n",
    "        self.ad = act_dim\n",
    "        self.logits = nn.Linear(d_model, comps)\n",
    "        self.means = nn.Linear(d_model, comps * act_dim)\n",
    "        self.log_stds = nn.Linear(d_model, comps * act_dim)\n",
    "    def forward(self, x):\n",
    "        B, T, _ = x.shape\n",
    "        logits = self.logits(x)\n",
    "        means = self.means(x).view(B, T, self.K, self.ad)\n",
    "        logstd = self.log_stds(x).view(B, T, self.K, self.ad)\n",
    "        stds = torch.exp(logstd.clamp(-5, 2))\n",
    "        return logits, means, stds\n",
    "\n",
    "\n",
    "class FormationGoalEncoder(nn.Module):\n",
    "    def __init__(self, goal_dim=13, d_model=256, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.enc = nn.Sequential(\n",
    "            nn.Linear(goal_dim, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model, d_model)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.enc(x)\n",
    "\n",
    "\n",
    "class SwarmTransformerWithGoal(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.odom_enc = ModalityEncoder(13, 256, 8, 4)\n",
    "        self.scan_enc = ModalityEncoder(361, 256, 8, 4)\n",
    "        self.gossip_enc = ModalityEncoder(70, 256, 8, 4)\n",
    "        self.goal_enc = FormationGoalEncoder(goal_dim=13, d_model=256)\n",
    "        self.cross1 = CrossModalAttention(256, 8)  # State + Goal\n",
    "        self.cross2 = CrossModalAttention(256, 8)  # Goal-aware + Perception\n",
    "        self.cross3 = CrossModalAttention(256, 8)  # Final + Communication\n",
    "        self.gmm_head = GMMHead(256, 2, comps=5)\n",
    "    def forward(self, odom, scan, gossip, formation_goal, smask, gmask):\n",
    "        o = self.odom_enc(odom)              # [B, T, 256]\n",
    "        s = self.scan_enc(scan, smask)       # [B, T, 256]\n",
    "        g = self.gossip_enc(gossip, gmask)   # [B, T, 256]\n",
    "        fg = self.goal_enc(formation_goal)   # [B, T, 256]\n",
    "\n",
    "        # Stage 1: Fuse state with goal\n",
    "        goal_aware_state = self.cross1(o, fg, fg, None)\n",
    "        # Stage 2: Fuse with perception\n",
    "        perception_fusion = self.cross2(goal_aware_state, s, s, smask)\n",
    "        # Stage 3: Fuse with communication/gossip\n",
    "        full = self.cross3(perception_fusion, g, g, gmask)\n",
    "\n",
    "        return self.gmm_head(full)\n",
    "    def loss(self, logits, means, stds, tgt):\n",
    "        B, T, K, D = means.shape\n",
    "        lf = logits.view(-1, K)\n",
    "        mf = means.view(-1, K, D)\n",
    "        sf = stds.view(-1, K, D)\n",
    "        tf = tgt.view(-1, D)\n",
    "        mix = torch.distributions.Categorical(logits=lf)\n",
    "        comp = torch.distributions.Independent(torch.distributions.Normal(mf, sf), 1)\n",
    "        gmm = torch.distributions.MixtureSameFamily(mix, comp)\n",
    "        return -gmm.log_prob(tf).mean()\n",
    "\n",
    "\n",
    "class SwarmRobotDataset(Dataset):\n",
    "    def __init__(self, files, max_nb=10, scan_dim=360, seq_len=None, normalize=True):\n",
    "        self.files = files\n",
    "        self.max_nb = max_nb\n",
    "        self.scan_dim = scan_dim\n",
    "        self.seq_len = seq_len\n",
    "        self.normalize = normalize\n",
    "        self.samples = []\n",
    "        for f in files:\n",
    "            data = np.load(f, allow_pickle=True)\n",
    "            for robot in data.files:\n",
    "                self.samples.append((f, robot))\n",
    "        assert self.samples, \"No samples loaded\"\n",
    "        if self.normalize:\n",
    "            self._compute_stats()\n",
    "            logger.info(f\"Computed stats on {len(self.samples)} samples\")\n",
    "            logger.info(f\"odom_mean: {self.om.tolist()}, odom_std: {self.os.tolist()}\")\n",
    "            logger.info(f\"scan_mean (first5): {self.sm.tolist()[:5]}, scan_std (first5): {self.ss.tolist()[:5]}\")\n",
    "            logger.info(f\"gossip_mean: {self.gm.tolist()}, gossip_std: {self.gs.tolist()}\")\n",
    "            logger.info(f\"cmd_vel_mean: {self.cm.tolist()}, cmd_vel_std: {self.cs.tolist()}\")\n",
    "\n",
    "    def _compute_stats(self):\n",
    "        od, sc, go, cmd = [], [], [], []\n",
    "        for f, robot in random.sample(self.samples, min(len(self.samples), 1000)):\n",
    "            odom, scan, gossip, ms, mg, cmdv = self._load_one(f, robot)\n",
    "            od.append(odom)\n",
    "            sc.append(scan)\n",
    "            go.append(gossip)\n",
    "            cmd.append(cmdv)\n",
    "        od = torch.cat(od, 0)\n",
    "        sc = torch.cat(sc, 0)\n",
    "        go = torch.cat(go, 0)\n",
    "        cmd = torch.cat(cmd, 0)\n",
    "        self.om, self.os = od.mean(0), od.std(0) + 1e-8\n",
    "        self.sm, self.ss = sc.mean(0), sc.std(0) + 1e-8\n",
    "        self.gm, self.gs = go.mean(0), go.std(0) + 1e-8\n",
    "        self.cm, self.cs = cmd.mean(0), cmd.std(0) + 1e-8\n",
    "\n",
    "    def _load_one(self, f, robot):\n",
    "        data = np.load(f, allow_pickle=True)\n",
    "        rd = data[robot].item()\n",
    "        odom = torch.tensor(rd['odom'], dtype=torch.float32)\n",
    "        cmdv = torch.tensor(rd['cmd_vel'], dtype=torch.float32)\n",
    "        scan_list, mask_s = [], []\n",
    "        for r in rd['scan']:\n",
    "            if r is None:\n",
    "                scan_list.append(np.zeros(self.scan_dim + 1))\n",
    "                mask_s.append(True)\n",
    "            else:\n",
    "                array = r.astype(np.float32)\n",
    "                valid = array[np.isfinite(array)]\n",
    "                range_max = valid.max() if valid.size > 0 else 1.0\n",
    "                cvar = compute_cvar(array, float(range_max))\n",
    "                array = np.where(np.isinf(array), range_max, array) / range_max\n",
    "                if array.size != self.scan_dim:\n",
    "                    array = np.interp(np.linspace(0, array.size - 1, self.scan_dim),\n",
    "                                      np.arange(array.size), array)\n",
    "                scan_list.append(np.concatenate(([cvar], array)))\n",
    "                mask_s.append(False)\n",
    "        scan = torch.tensor(np.vstack(scan_list), dtype=torch.float32)\n",
    "        mask_s = torch.tensor(mask_s)\n",
    "        go_list, mask_g = [], []\n",
    "        for g in rd['gossip']:\n",
    "            if g.size == 0:\n",
    "                go_list.append(np.zeros(self.max_nb * 7))\n",
    "                mask_g.append(True)\n",
    "            else:\n",
    "                arr = np.zeros(self.max_nb * 7)\n",
    "                for i, row in enumerate(g[:self.max_nb]):\n",
    "                    arr[i * 7:(i + 1) * 7] = row\n",
    "                go_list.append(arr)\n",
    "                mask_g.append(False)\n",
    "        gossip = torch.tensor(np.vstack(go_list), dtype=torch.float32)\n",
    "        mask_g = torch.tensor(mask_g)\n",
    "        return odom, scan, gossip, mask_s, mask_g, cmdv\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        f, robot = self.samples[idx]\n",
    "        odom, scan, gossip, ms, mg, cmdv = self._load_one(f, robot)\n",
    "        L = odom.size(0)\n",
    "        if self.seq_len and L > self.seq_len:\n",
    "            i = random.randint(0, L - self.seq_len)\n",
    "            odom, scan, gossip, ms, mg, cmdv = [\n",
    "                x[i:i + self.seq_len] for x in (odom, scan, gossip, ms, mg, cmdv)\n",
    "            ]\n",
    "        if self.normalize:\n",
    "            odom = (odom - self.om) / self.os\n",
    "            scan = (scan - self.sm) / self.ss\n",
    "            gossip = (gossip - self.gm) / self.gs\n",
    "            cmdv = (cmdv - self.cm) / self.cs\n",
    "\n",
    "        formation_goal_vec = odom[-1].unsqueeze(0).repeat(odom.size(0), 1)  # [L, 13]\n",
    "        return {\n",
    "            'odom': odom, 'scan': scan, 'gossip': gossip,\n",
    "            'scan_mask': ms, 'gossip_mask': mg, 'cmd_vel': cmdv,\n",
    "            'formation_goal': formation_goal_vec\n",
    "        }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    B = len(batch)\n",
    "    L = max(x['odom'].size(0) for x in batch)\n",
    "    odom = torch.zeros(B, L, 13)\n",
    "    scan = torch.zeros(B, L, 361)\n",
    "    gossip = torch.zeros(B, L, 70)\n",
    "    sm = torch.ones(B, L, dtype=torch.bool)\n",
    "    gm = torch.ones(B, L, dtype=torch.bool)\n",
    "    cmdv = torch.zeros(B, L, 2)\n",
    "    formation_goal = torch.zeros(B, L, 13)\n",
    "    for i, x in enumerate(batch):\n",
    "        l = x['odom'].size(0)\n",
    "        odom[i, :l] = x['odom']\n",
    "        scan[i, :l] = x['scan']\n",
    "        gossip[i, :l] = x['gossip']\n",
    "        sm[i, :l] = x['scan_mask']\n",
    "        gm[i, :l] = x['gossip_mask']\n",
    "        cmdv[i, :l] = x['cmd_vel']\n",
    "        formation_goal[i, :l] = x['formation_goal']\n",
    "    return {\n",
    "        'odom': odom, 'scan': scan, 'gossip': gossip,\n",
    "        'scan_mask': sm, 'gossip_mask': gm, 'cmd_vel': cmdv,\n",
    "        'formation_goal': formation_goal\n",
    "    }\n",
    "\n",
    "\n",
    "class SwarmTrainer:\n",
    "    def __init__(self, model, tl, vl, te, device):\n",
    "        self.m, self.tl, self.vl, self.te = model.to(device), tl, vl, te\n",
    "        self.dev = device\n",
    "        self.opt = torch.optim.AdamW(model.parameters(), lr=1e-5, weight_decay=1e-4)\n",
    "        self.sch = torch.optim.lr_scheduler.CosineAnnealingLR(self.opt, T_max=100)\n",
    "        self.best = float('inf')\n",
    "        self.train_losses, self.val_losses = [], []\n",
    "        self.train_maes, self.val_maes = [], []\n",
    "        self.train_rmses, self.val_rmses = [], []\n",
    "\n",
    "    def _run(self, loader, train):\n",
    "        self.m.train() if train else self.m.eval()\n",
    "        total_loss, total_mae, total_rmse, n_samples = 0.0, 0.0, 0.0, 0\n",
    "        for b in tqdm(loader, desc=\"Train\" if train else \"Val\"):\n",
    "            od, sc, go, fg, sm, gm, cmd = [b[k].to(self.dev) for k in\n",
    "                                           ('odom', 'scan', 'gossip', 'formation_goal', 'scan_mask', 'gossip_mask', 'cmd_vel')]\n",
    "            with torch.set_grad_enabled(train):\n",
    "                logits, means, stds = self.m(od, sc, go, fg, sm, gm)\n",
    "                loss = self.m.loss(logits, means, stds, cmd)\n",
    "                if train:\n",
    "                    self.opt.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(self.m.parameters(), 1.0)\n",
    "                    self.opt.step()\n",
    "            total_loss += loss.item()\n",
    "            n_samples += 1\n",
    "            maxidx = logits.argmax(dim=-1)\n",
    "            pred = torch.gather(means, 2, maxidx.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, 1, means.size(-1))).squeeze(2)\n",
    "            total_mae += (pred - cmd).abs().mean().item()\n",
    "            total_rmse += ((pred - cmd) ** 2).mean().sqrt().item()\n",
    "        avg_loss = total_loss / n_samples\n",
    "        avg_mae = total_mae / n_samples\n",
    "        avg_rmse = total_rmse / n_samples\n",
    "        return avg_loss, avg_mae, avg_rmse\n",
    "\n",
    "    def save_checkpoint(self, epoch, path):\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state': self.m.state_dict(),\n",
    "            'optimizer_state': self.opt.state_dict(),\n",
    "            'scheduler_state': self.sch.state_dict(),\n",
    "            'best': self.best\n",
    "        }, path)\n",
    "\n",
    "    def train(self, epochs=100):\n",
    "        for e in range(81, epochs + 1):\n",
    "            tr_loss, tr_mae, tr_rmse = self._run(self.tl, True)\n",
    "            val_loss, val_mae, val_rmse = self._run(self.vl, False)\n",
    "            self.train_losses.append(tr_loss)\n",
    "            self.val_losses.append(val_loss)\n",
    "            self.train_maes.append(tr_mae)\n",
    "            self.val_maes.append(val_mae)\n",
    "            self.train_rmses.append(tr_rmse)\n",
    "            self.val_rmses.append(val_rmse)\n",
    "\n",
    "            if val_loss < self.best:\n",
    "                self.best = val_loss\n",
    "                checkpoint_path = f\"best_form_model_{e}.pth\"\n",
    "                self.save_checkpoint(e, checkpoint_path)\n",
    "\n",
    "            self.sch.step()\n",
    "            print()\n",
    "            print(f\"Epoch: {e}/{epochs} | Train Loss: {tr_loss:.4f} | Val Loss: {val_loss:.4f} \"\n",
    "                  f\"Train MAE: {tr_mae:.4f} | Val MAE:{val_mae:.4f} \"\n",
    "                  f\"Train RMSE {tr_rmse:.4f} | Val RMSE {val_rmse:.4f}\", flush=True)\n",
    "            logger.info(f\"Epoch {e}/{epochs} Train Loss {tr_loss:.4f} Val Loss {val_loss:.4f} \"\n",
    "                        f\"Train MAE {tr_mae:.4f} Val MAE {val_mae:.4f} \"\n",
    "                        f\"Train RMSE {tr_rmse:.4f} Val RMSE {val_rmse:.4f}\")\n",
    "\n",
    "        plt.figure(); plt.plot(self.train_losses, label='Train'); plt.plot(self.val_losses, label='Val'); plt.legend()\n",
    "        plt.savefig(\"training_curve.png\")\n",
    "        te_loss, te_mae, te_rmse = self._run(self.te, False)\n",
    "        logger.info(f\"Test Loss: {te_loss:.4f} | Test MAE: {te_mae:.4f} | Test RMSE: {te_rmse:.4f}\")\n",
    "\n",
    "    def load_model_only(self, path):\n",
    "        checkpoint = torch.load(path, map_location=self.dev)\n",
    "        self.m.load_state_dict(checkpoint['model_state'])\n",
    "        self.opt.load_state_dict(checkpoint['optimizer_state'])\n",
    "        self.sch.load_state_dict(checkpoint['scheduler_state'])\n",
    "        print(f\"Loaded model weights from {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cfb673",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_test_set(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_mae = 0.0\n",
    "    total_rmse = 0.0\n",
    "    n_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating Test Set\"):\n",
    "            odom = batch['odom'].to(device)\n",
    "            scan = batch['scan'].to(device)\n",
    "            gossip = batch['gossip'].to(device)\n",
    "            formation_goal = batch['formation_goal'].to(device)\n",
    "            scan_mask = batch['scan_mask'].to(device)\n",
    "            gossip_mask = batch['gossip_mask'].to(device)\n",
    "            cmd_vel = batch['cmd_vel'].to(device)\n",
    "\n",
    "            logits, means, stds = model(odom, scan, gossip, formation_goal, scan_mask, gossip_mask)\n",
    "            loss = model.loss(logits, means, stds, cmd_vel)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            maxidx = logits.argmax(dim=-1)\n",
    "            pred = torch.gather(means, 2, maxidx.unsqueeze(-1).unsqueeze(-1).expand(-1, -1, 1, means.size(-1))).squeeze(2)\n",
    "\n",
    "            total_mae += (pred - cmd_vel).abs().mean().item()\n",
    "            total_rmse += ((pred - cmd_vel) ** 2).mean().sqrt().item()\n",
    "            n_samples += 1\n",
    "\n",
    "    avg_loss = total_loss / n_samples\n",
    "    avg_mae = total_mae / n_samples\n",
    "    avg_rmse = total_rmse / n_samples\n",
    "\n",
    "    print(f\"Test Loss (NLL): {avg_loss:.4f}\")\n",
    "    print(f\"Test MAE: {avg_mae:.4f}\")\n",
    "    print(f\"Test RMSE: {avg_rmse:.4f}\")\n",
    "\n",
    "    return avg_loss, avg_mae, avg_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40377de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"------------------ Starting Code -----------------------\")\n",
    "print(\"Device : \", DEVICE)\n",
    "DATA_DIR = \"/content/converted_datasets\"\n",
    "files = list(Path(DATA_DIR).glob(\"*.npz\"))\n",
    "tr, tmp = train_test_split(files, test_size=0.3, random_state=42)\n",
    "vl, te = train_test_split(tmp, test_size=0.5, random_state=42)\n",
    "print(\"Splits made:-\")\n",
    "print(f\"Train: {len(tr)} | Val: {len(vl)} | Test: {len(te)}\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Creating Dataset Classes.\")\n",
    "train_ds = SwarmRobotDataset(tr, seq_len=3500, normalize=True)\n",
    "print(\"Train Dataset Class created.\")\n",
    "val_ds = SwarmRobotDataset(vl, seq_len=3500, normalize=False)\n",
    "print(\"Val Dataset Class created.\")\n",
    "test_ds = SwarmRobotDataset(te, seq_len=3500, normalize=False)\n",
    "print(\"Test Dataset Class created.\")\n",
    "print((f\"Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\"))\n",
    "for ds in (val_ds, test_ds):\n",
    "    ds.om, ds.os = train_ds.om, train_ds.os\n",
    "    ds.sm, ds.ss = train_ds.sm, train_ds.ss\n",
    "    ds.gm, ds.gs = train_ds.gm, train_ds.gs\n",
    "    ds.cm, ds.cs = train_ds.cm, train_ds.cs\n",
    "    ds.normalize = True\n",
    "\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "model = SwarmTransformerWithGoal()\n",
    "logger.info(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Starting Training.\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "checkpoint_path = \"best_form_model_99.pth\" # Specify the correct checkpoint path here\n",
    "\n",
    "# Load the checkpoint dictionary\n",
    "checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "\n",
    "# Load the model state from the checkpoint\n",
    "model.load_state_dict(checkpoint['model_state'] if 'model_state' in checkpoint else checkpoint)\n",
    "model.to(DEVICE)\n",
    "\n",
    "evaluate_test_set(model, test_loader, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c54254",
   "metadata": {},
   "source": [
    "MLP BASELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf61a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "class MLPPolicy(nn.Module):\n",
    "    def __init__(self, input_dim, goal_dim=13, hidden_sizes=[128, 128], output_dim=2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        in_dim = input_dim + goal_dim\n",
    "        for h in hidden_sizes:\n",
    "            layers.append(nn.Linear(in_dim, h))\n",
    "            layers.append(nn.ReLU())\n",
    "            in_dim = h\n",
    "        layers.append(nn.Linear(in_dim, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, odom, scan, gossip, formation_goal):\n",
    "        B, T, _ = odom.shape\n",
    "        x = torch.cat([odom, scan, gossip, formation_goal], dim=-1)\n",
    "        x = x.view(B * T, -1)\n",
    "        out = self.net(x)\n",
    "        return out.view(B, T, -1)\n",
    "\n",
    "\n",
    "def validate_mlp(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, total_mae, total_rmse, n = 0, 0, 0, 0\n",
    "    criterion = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            odom = batch['odom'].to(device)\n",
    "            scan = batch['scan'].to(device)\n",
    "            gossip = batch['gossip'].to(device)\n",
    "            fg = batch['formation_goal'].to(device)\n",
    "            cmd = batch['cmd_vel'].to(device)\n",
    "\n",
    "            pred = model(odom, scan, gossip, fg)\n",
    "            loss = criterion(pred, cmd)\n",
    "            total_loss += loss.item()\n",
    "            total_mae += (pred - cmd).abs().mean().item()\n",
    "            total_rmse += ((pred - cmd) ** 2).mean().sqrt().item()\n",
    "            n += 1\n",
    "    avg_loss = total_loss / n\n",
    "    print(f\"Val MSE: {avg_loss:.6f}, MAE: {total_mae/n:.6f}, RMSE: {total_rmse/n:.6f}\")\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def train_mlp(model, train_loader, val_loader, device, epochs=30, lr=1e-4, save_path=\"\"):\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "            odom = batch['odom'].to(device)\n",
    "            scan = batch['scan'].to(device)\n",
    "            gossip = batch['gossip'].to(device)\n",
    "            fg = batch['formation_goal'].to(device)\n",
    "            cmd = batch['cmd_vel'].to(device)\n",
    "\n",
    "            pred = model(odom, scan, gossip, fg)\n",
    "            loss = criterion(pred, cmd)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} Train MSE: {avg_train_loss:.6f}\")\n",
    "\n",
    "        val_loss = validate_mlp(model, val_loader, device)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f\"best_mlp_{epoch + 1}.pth\")\n",
    "            print(f\"Saved best model with val loss {best_val_loss:.6f} at epoch {epoch+1}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68081248",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "# MLP baseline\n",
    "mlp_model = MLPPolicy(input_dim=444, goal_dim=13)\n",
    "trained_mlp = train_mlp(mlp_model, train_loader, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c252d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mlp_on_test(model, test_loader, device, checkpoint_path=None):\n",
    "    # Optionally load the best model weights saved during training\n",
    "    if checkpoint_path is not None:\n",
    "        model.load_state_dict(torch.load(checkpoint_path, map_location=device))\n",
    "        print(f\"Loaded model weights from {checkpoint_path}\")\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    total_loss, total_mae, total_rmse, n = 0, 0, 0, 0\n",
    "    criterion = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Test Evaluation\"):\n",
    "            odom = batch['odom'].to(device)\n",
    "            scan = batch['scan'].to(device)\n",
    "            gossip = batch['gossip'].to(device)\n",
    "            fg = batch['formation_goal'].to(device)\n",
    "            cmd = batch['cmd_vel'].to(device)\n",
    "\n",
    "            pred = model(odom, scan, gossip, fg)\n",
    "            loss = criterion(pred, cmd)\n",
    "            total_loss += loss.item()\n",
    "            total_mae += (pred - cmd).abs().mean().item()\n",
    "            total_rmse += ((pred - cmd) ** 2).mean().sqrt().item()\n",
    "            n += 1\n",
    "\n",
    "    print(f\"\\nTest MSE: {total_loss/n:.6f}\\nTest MAE: {total_mae/n:.6f}\\nTest RMSE: {total_rmse/n:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca109040",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"------------------ Starting Code -----------------------\")\n",
    "print(\"Device : \", DEVICE)\n",
    "DATA_DIR = \"/content/converted_datasets\"\n",
    "files = list(Path(DATA_DIR).glob(\"*.npz\"))\n",
    "tr, tmp = train_test_split(files, test_size=0.3, random_state=42)\n",
    "vl, te = train_test_split(tmp, test_size=0.5, random_state=42)\n",
    "print(\"Splits made:-\")\n",
    "print(f\"Train: {len(tr)} | Val: {len(vl)} | Test: {len(te)}\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Creating Dataset Classes.\")\n",
    "train_ds = SwarmRobotDataset(tr, seq_len=3500, normalize=True)\n",
    "print(\"Train Dataset Class created.\")\n",
    "val_ds = SwarmRobotDataset(vl, seq_len=3500, normalize=False)\n",
    "print(\"Val Dataset Class created.\")\n",
    "test_ds = SwarmRobotDataset(te, seq_len=3500, normalize=False)\n",
    "print(\"Test Dataset Class created.\")\n",
    "print((f\"Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\"))\n",
    "for ds in (val_ds, test_ds):\n",
    "    ds.om, ds.os = train_ds.om, train_ds.os\n",
    "    ds.sm, ds.ss = train_ds.sm, train_ds.ss\n",
    "    ds.gm, ds.gs = train_ds.gm, train_ds.gs\n",
    "    ds.cm, ds.cs = train_ds.cm, train_ds.cs\n",
    "    ds.normalize = True\n",
    "\n",
    "test_loader = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "model = MLPPolicy(input_dim=444, goal_dim=13)\n",
    "logger.info(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Starting Training.\")\n",
    "print(\"--------------------------------------------------------\")\n",
    "model.load_state_dict(torch.load(\"best_mlp_29.pth\"))\n",
    "model.to(DEVICE)\n",
    "\n",
    "evaluate_mlp_on_test(model, test_loader, DEVICE, checkpoint_path=\"best_mlp_29.pth\") # Load best model weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e0d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "class RNNPolicy(nn.Module):\n",
    "    def __init__(self, input_dim, goal_dim=13, hidden_dim=128, num_layers=1, output_dim=2):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(input_dim + goal_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, odom, scan, gossip, formation_goal):\n",
    "        x = torch.cat([odom, scan, gossip, formation_goal], dim=-1)  # [B, T, input_dim+goal_dim]\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "def validate_rnn(model, val_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, total_mae, total_rmse, n = 0, 0, 0, 0\n",
    "    criterion = nn.MSELoss()\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            odom = batch['odom'].to(device)\n",
    "            scan = batch['scan'].to(device)\n",
    "            gossip = batch['gossip'].to(device)\n",
    "            fg = batch['formation_goal'].to(device)\n",
    "            cmd = batch['cmd_vel'].to(device)\n",
    "\n",
    "            pred = model(odom, scan, gossip, fg)\n",
    "            loss = criterion(pred, cmd)\n",
    "            total_loss += loss.item()\n",
    "            total_mae += (pred - cmd).abs().mean().item()\n",
    "            total_rmse += ((pred - cmd) ** 2).mean().sqrt().item()\n",
    "            n += 1\n",
    "    avg_loss = total_loss / n\n",
    "    avg_mae = total_mae / n\n",
    "    avg_rmse = total_rmse / n\n",
    "    print(f\"Val MSE: {avg_loss:.6f}, MAE: {avg_mae:.6f}, RMSE: {avg_rmse:.6f}\")\n",
    "    return avg_loss\n",
    "\n",
    "def train_rnn(model, train_loader, val_loader, device, epochs=30, lr=1e-4, save_path=\"\"):\n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "            odom = batch['odom'].to(device)\n",
    "            scan = batch['scan'].to(device)\n",
    "            gossip = batch['gossip'].to(device)\n",
    "            fg = batch['formation_goal'].to(device)\n",
    "            cmd = batch['cmd_vel'].to(device)\n",
    "\n",
    "            pred = model(odom, scan, gossip, fg)\n",
    "            loss = criterion(pred, cmd)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1} Train MSE: {avg_train_loss:.6f}\")\n",
    "\n",
    "        val_loss = validate_rnn(model, val_loader, device)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), f\"best_rnn_{epoch + 1}.pth\")\n",
    "            print(f\"Saved best model with val loss {best_val_loss:.6f} at epoch {epoch + 1}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def main():\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(\"------------------ Starting RNN Training -----------------------\")\n",
    "    print(\"Device:\", DEVICE)\n",
    "    DATA_DIR = \"/content/converted_datasets\"  # update to your dataset path\n",
    "    files = list(Path(DATA_DIR).glob(\"*.npz\"))\n",
    "    tr, tmp = train_test_split(files, test_size=0.3, random_state=42)\n",
    "    vl, te = train_test_split(tmp, test_size=0.5, random_state=42)\n",
    "    print(\"Splits made:-\")\n",
    "    print(f\"Train: {len(tr)} | Val: {len(vl)} | Test: {len(te)}\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "\n",
    "    train_ds = SwarmRobotDataset(tr, seq_len=256, normalize=True)\n",
    "    val_ds = SwarmRobotDataset(vl, seq_len=256, normalize=True)\n",
    "    test_ds = SwarmRobotDataset(te, seq_len=256, normalize=True)\n",
    "\n",
    "    # Share normalization stats\n",
    "    for ds in (val_ds, test_ds):\n",
    "        ds.om, ds.os = train_ds.om, train_ds.os\n",
    "        ds.sm, ds.ss = train_ds.sm, train_ds.ss\n",
    "        ds.gm, ds.gs = train_ds.gm, train_ds.gs\n",
    "        ds.cm, ds.cs = train_ds.cm, train_ds.cs\n",
    "        ds.normalize = True\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "    test_loader = DataLoader(test_ds, batch_size=4, shuffle=False, collate_fn=collate_fn, num_workers=0)\n",
    "\n",
    "    input_dim = 13 + 361 + 70  # odom + scan + gossip features\n",
    "    model = RNNPolicy(input_dim=input_dim, goal_dim=13, hidden_dim=128, num_layers=1)\n",
    "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "    print(\"Starting Training.\")\n",
    "    print(\"--------------------------------------------------------\")\n",
    "\n",
    "    # Train and validate\n",
    "    trained_model = train_rnn(model, train_loader, val_loader, DEVICE, epochs=30, lr=1e-4, save_path=\"rnn_best.pth\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987797e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_rnn(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_loss, total_mae, total_rmse, n = 0, 0, 0, 0\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_loader, desc=\"Evaluating Test Set\"):\n",
    "            odom = batch['odom'].to(device)\n",
    "            scan = batch['scan'].to(device)\n",
    "            gossip = batch['gossip'].to(device)\n",
    "            fg = batch['formation_goal'].to(device)\n",
    "            cmd = batch['cmd_vel'].to(device)\n",
    "\n",
    "            pred = model(odom, scan, gossip, fg)\n",
    "            loss = criterion(pred, cmd)\n",
    "            total_loss += loss.item()\n",
    "            total_mae += (pred - cmd).abs().mean().item()\n",
    "            total_rmse += ((pred - cmd)**2).mean().sqrt().item()\n",
    "            n += 1\n",
    "\n",
    "    avg_loss = total_loss / n\n",
    "    avg_mae = total_mae / n\n",
    "    avg_rmse = total_rmse / n\n",
    "\n",
    "    print(f\"Test MSE: {avg_loss:.6f}\")\n",
    "    print(f\"Test MAE: {avg_mae:.6f}\")\n",
    "    print(f\"Test RMSE: {avg_rmse:.6f}\")\n",
    "\n",
    "    return avg_loss, avg_mae, avg_rmse\n",
    "\n",
    "# Usage example:\n",
    "# Load your RNN model and best checkpoint first:\n",
    "input_dim = 13 + 361 + 70\n",
    "model = RNNPolicy(input_dim=input_dim, goal_dim=13, hidden_dim=128, num_layers=1)\n",
    "model.load_state_dict(torch.load(\"best_rnn_29.pth\")) # Specify the correct checkpoint path\n",
    "model.to(device)\n",
    "evaluate_rnn(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
